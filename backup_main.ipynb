{"cells":[{"cell_type":"markdown","metadata":{"id":"vobP7ahXP1EG"},"source":["# Graph-Learning-Based Recommender System on MovieLens\n","\n","### Group 9\n","\n","- AGARWAL, Sahil\n","- WEI, Yuanjing\n","- ZHANG, Yujun yzhanglo@connect.ust.hk\n","\n","Group project of COMP4222@HKUST in 2022 Fall."]},{"cell_type":"markdown","metadata":{"id":"QpTqYz7qYJbP"},"source":["# 1 Environment Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":603},"executionInfo":{"elapsed":30299,"status":"error","timestamp":1667642237984,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"MXxR7IpwI83o","outputId":"9933f848-12fa-442f-afe4-0d95a20a464e"},"outputs":[{"name":"stdout","output_type":"stream","text":["/data/yzhanglo/4222project\n","backup_main.ipynb  LICENSE        \u001b[0m\u001b[38;5;27mml-100k\u001b[0m/          params.csv\n","\u001b[38;5;27mcomp4222\u001b[0m/          \u001b[38;5;27mLightGCN\u001b[0m/      \u001b[38;5;27mml-latest-small\u001b[0m/  \u001b[38;5;27mrecommenders\u001b[0m/\n","cuda101.yml        lightgcn.yaml  movielens.ipynb   requirements.txt\n","\u001b[38;5;27mKGAT_folder\u001b[0m/       main.ipynb     \u001b[38;5;9mmovielens.zip\u001b[0m\n"]}],"source":["# change the path in the following\n","try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    %cd '/content/drive/MyDrive/4222Group9'\n","except:\n","    %cd '/data/yzhanglo/4222project'\n","\n","import comp4222\n","import recommenders\n","%pwd\n","%ls"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"aborted","timestamp":1667642237987,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"7RuvfqktV-jm"},"outputs":[{"name":"stderr","output_type":"stream","text":["/data/yzhanglo/miniconda/envs/cu101/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","2022-11-22 11:41:33.947983: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","from tensorboardX import SummaryWriter\n","\n","import tensorflow as tf\n","tf.get_logger().setLevel('ERROR') # only show error messages\n","\n","# easier to print by putting variable as a single line\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","# make matplotlib figures appear inline in the notebook rather than in a new window.\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# Add some convenience functions to Pandas DataFrame.\n","pd.options.display.max_rows = 10\n","pd.options.display.float_format = '{:.2f}'.format\n","def mask(df, key, function):\n","  \"\"\"Returns a filtered dataframe, by applying function to key\"\"\"\n","  return df[function(df[key])]\n","\n","def flatten_cols(df):\n","  df.columns = [' '.join(col).strip() for col in df.columns.values]\n","  return df\n","\n","pd.DataFrame.mask = mask\n","pd.DataFrame.flatten_cols = flatten_cols\n","\n","# http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"aborted","timestamp":1667642237988,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"zLFdbrlyOWiT"},"outputs":[{"name":"stdout","output_type":"stream","text":["hahaha\n"]}],"source":["# Testing the module functionality\n","from comp4222 import b\n","comp4222.b.ok()"]},{"cell_type":"markdown","metadata":{"id":"dkgjzCmnWXUw"},"source":["# 2 MovieLens\n"]},{"cell_type":"markdown","metadata":{"id":"Mq_PrL0LZrGa"},"source":["We're using ml-latest-small from MovieLens. It contains 100836 ratings and 3683 tag applications across 9742 movies. These data were created by 610 users between March 29, 1996 and September 24, 2018. This dataset was generated on September 26, 2018. The readme.md is avaliable [here](https://files.grouplens.org/datasets/movielens/ml-latest-small-README.html)."]},{"cell_type":"markdown","metadata":{"id":"g_XRmlt-yMUu"},"source":["## Data Loading for ml-latest-small"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"aborted","timestamp":1667642237989,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"Vt7THxaKWZDv"},"outputs":[],"source":["# Download MovieLens data.\n","dataset_name = \"ml-latest-small\"\n","from urllib.request import urlretrieve\n","import zipfile\n","urlretrieve(f\"https://files.grouplens.org/datasets/movielens/{dataset_name}.zip\", \"movielens.zip\")\n","zipfile.ZipFile(\"movielens.zip\", \"r\").extractall()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"aborted","timestamp":1667642237989,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"vhrmXmjSh5jf"},"outputs":[],"source":["movies = pd.read_csv(f\"{dataset_name}/movies.csv\")\n","genre_cols = [\n","    \"(no genres listed)\", \"Action\", \"Adventure\", \"Animation\", \"Children\", \"Comedy\",\n","    \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\",\n","    \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"\n","]\n","movies"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"aborted","timestamp":1667642237990,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"HipV8C2ziAUh"},"outputs":[],"source":["tags = pd.read_csv(f\"{dataset_name}/tags.csv\")\n","tags"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"aborted","timestamp":1667642237990,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"RF0KOsiadLM1"},"outputs":[],"source":["ratings = pd.read_csv(f\"{dataset_name}/ratings.csv\")\n","ratings"]},{"cell_type":"markdown","metadata":{"id":"OzxFHwzLy2vU"},"source":["# 5 Backup Models Definition"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":377},"executionInfo":{"elapsed":473,"status":"error","timestamp":1667642260421,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"U0Q4UEcMy6g6","outputId":"62577a20-91f8-43cf-8ea8-5d9138fa711a"},"outputs":[],"source":["from recommenders.utils.timer import Timer\n","from recommenders.models.deeprec.deeprec_utils import prepare_hparams\n","from recommenders.utils.constants import (\n","    COL_DICT,\n","    DEFAULT_K,\n","    DEFAULT_USER_COL,\n","    DEFAULT_ITEM_COL,\n","    DEFAULT_RATING_COL,\n","    DEFAULT_PREDICTION_COL,\n","    DEFAULT_TIMESTAMP_COL,\n","    SEED,\n",")\n","\n","# Helpers\n","import os\n","from tempfile import TemporaryDirectory\n","tmp_dir = TemporaryDirectory()\n","TRAIN_FILE = os.path.join(tmp_dir.name, \"df_train.csv\")\n","TEST_FILE = os.path.join(tmp_dir.name, \"df_test.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":377},"executionInfo":{"elapsed":6,"status":"error","timestamp":1667642261027,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"kBSE8h2MzB16","outputId":"cfd4e3c0-5af3-4a2d-f1ea-dddfcadbd897"},"outputs":[],"source":["from recommenders.utils.spark_utils import start_or_get_spark\n","spark = start_or_get_spark(\"PySpark\", memory=\"32g\")\n","spark.conf.set(\"spark.sql.analyzer.failAmbiguousSelfJoin\", \"false\")\n","\n","from recommenders.datasets import movielens\n","from recommenders.datasets.python_splitters import python_stratified_split\n","\n","# fix random seeds to make sure out runs are reproducible\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)"]},{"cell_type":"markdown","metadata":{},"source":["### ALS"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pyspark.sql.types import StructType, StructField\n","from pyspark.sql.types import FloatType, IntegerType, LongType\n","from pyspark.ml.recommendation import ALS\n","\n","\n","def prepare_training_als(train, test):\n","    schema = StructType(\n","        (\n","            StructField(DEFAULT_USER_COL, IntegerType()),\n","            StructField(DEFAULT_ITEM_COL, IntegerType()),\n","            StructField(DEFAULT_RATING_COL, FloatType()),\n","            StructField(DEFAULT_TIMESTAMP_COL, LongType()),\n","        )\n","    )\n","    spark = start_or_get_spark()\n","    return spark.createDataFrame(train, schema).cache()\n","\n","def prepare_metrics_als(train, test):\n","    schema = StructType(\n","        (\n","            StructField(DEFAULT_USER_COL, IntegerType()),\n","            StructField(DEFAULT_ITEM_COL, IntegerType()),\n","            StructField(DEFAULT_RATING_COL, FloatType()),\n","            StructField(DEFAULT_TIMESTAMP_COL, LongType()),\n","        )\n","    )\n","    spark = start_or_get_spark()\n","    return spark.createDataFrame(train, schema).cache(), spark.createDataFrame(test, schema).cache()\n","\n","def predict_als(model, test):\n","    with Timer() as t:\n","        preds = model.transform(test)\n","    return preds, t\n","\n","def train_als(params, data):\n","    symbol = ALS(**params)\n","    with Timer() as t:\n","        model = symbol.fit(data)\n","    return model, t\n","\n","def recommend_k_als(model, test, train, top_k=DEFAULT_K, remove_seen=True):\n","    with Timer() as t:\n","        # Get the cross join of all user-item pairs and score them.\n","        users = train.select(DEFAULT_USER_COL).distinct()\n","        items = train.select(DEFAULT_ITEM_COL).distinct()\n","        user_item = users.crossJoin(items)\n","        dfs_pred = model.transform(user_item)\n","\n","        # Remove seen items\n","        dfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n","            train.alias(\"train\"),\n","            (dfs_pred[DEFAULT_USER_COL] == train[DEFAULT_USER_COL])\n","            & (dfs_pred[DEFAULT_ITEM_COL] == train[DEFAULT_ITEM_COL]),\n","            how=\"outer\",\n","        )\n","        topk_scores = dfs_pred_exclude_train.filter(\n","            dfs_pred_exclude_train[\"train.\" + DEFAULT_RATING_COL].isNull()\n","        ).select(\n","            \"pred.\" + DEFAULT_USER_COL,\n","            \"pred.\" + DEFAULT_ITEM_COL,\n","            \"pred.\" + DEFAULT_PREDICTION_COL,\n","        )\n","    return topk_scores, t\n","\n","\n","als_params = {\n","    \"rank\": 10,\n","    \"maxIter\": 20,\n","    \"implicitPrefs\": False,\n","    \"alpha\": 0.1,\n","    \"regParam\": 0.05,\n","    \"coldStartStrategy\": \"drop\",\n","    \"nonnegative\": False,\n","    \"userCol\": DEFAULT_USER_COL,\n","    \"itemCol\": DEFAULT_ITEM_COL,\n","    \"ratingCol\": DEFAULT_RATING_COL,\n","}"]},{"cell_type":"markdown","metadata":{"id":"M5QhzXoRr0b6"},"source":["### NCF"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1667642261028,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"28jUVetSy9As"},"outputs":[],"source":["from recommenders.models.ncf.ncf_singlenode import NCF\n","from recommenders.models.ncf.dataset import Dataset as NCFDataset\n","\n","def prepare_training_ncf(df_train, df_test):\n","    #df_train.sort_values([\"userID\"], axis=0, ascending=[True], inplace=True)\n","    #df_test.sort_values([\"userID\"], axis=0, ascending=[True], inplace=True)\n","    train = df_train.sort_values([\"userID\"], axis=0, ascending=[True])\n","    test = df_test.sort_values([\"userID\"], axis=0, ascending=[True])\n","    test = test[df_test[\"userID\"].isin(train[\"userID\"].unique())]\n","    test = test[test[\"itemID\"].isin(train[\"itemID\"].unique())]\n","    train.to_csv(TRAIN_FILE, index=False)\n","    test.to_csv(TEST_FILE, index=False)\n","    return NCFDataset(\n","        train_file=TRAIN_FILE,\n","        col_user=DEFAULT_USER_COL,\n","        col_item=DEFAULT_ITEM_COL,\n","        col_rating=DEFAULT_RATING_COL,\n","        seed=SEED,\n","    )\n","\n","\n","def train_ncf(params, data):\n","    model = NCF(n_users=data.n_users, n_items=data.n_items, **params)\n","    with Timer() as t:\n","        model.fit(data)\n","    return model, t\n","\n","\n","def recommend_k_ncf(model, test, train, top_k=DEFAULT_K, remove_seen=True):\n","    with Timer() as t:\n","        users, items, preds = [], [], []\n","        item = list(train[DEFAULT_ITEM_COL].unique())\n","        for user in train[DEFAULT_USER_COL].unique():\n","            user = [user] * len(item)\n","            users.extend(user)\n","            items.extend(item)\n","            preds.extend(list(model.predict(user, item, is_list=True)))\n","        topk_scores = pd.DataFrame(\n","            data={\n","                DEFAULT_USER_COL: users,\n","                DEFAULT_ITEM_COL: items,\n","                DEFAULT_PREDICTION_COL: preds,\n","            }\n","        )\n","        merged = pd.merge(\n","            train, topk_scores, on=[DEFAULT_USER_COL, DEFAULT_ITEM_COL], how=\"outer\"\n","        )\n","        topk_scores = merged[merged[DEFAULT_RATING_COL].isnull()].drop(\n","            DEFAULT_RATING_COL, axis=1\n","        )\n","    # Remove temp files\n","    return topk_scores, t\n","\n","ncf_params = {\n","    \"model_type\": \"NeuMF\",\n","    \"n_factors\": 4,\n","    \"layer_sizes\": [16, 8, 4],\n","    \"n_epochs\": 20,\n","    \"batch_size\": 1024,\n","    \"learning_rate\": 1e-3,\n","    \"verbose\": 10\n","}"]},{"cell_type":"markdown","metadata":{"id":"QsJmgwgNsBE8"},"source":["### LightGCN"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1667642261028,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"BZtl_jr3sA1o"},"outputs":[],"source":["from recommenders.models.deeprec.DataModel.ImplicitCF import ImplicitCF\n","from recommenders.models.deeprec.models.graphrec.lightgcn import LightGCN\n","    \n","def prepare_training_lightgcn(train, test):\n","    return ImplicitCF(train=train, test=test)\n","\n","def train_lightgcn(params, data):\n","    hparams = prepare_hparams(**params)\n","    model = LightGCN(hparams, data)\n","    with Timer() as t:\n","        model.fit()\n","    return model, t\n","\n","def recommend_k_lightgcn(model, test, train, top_k=DEFAULT_K, remove_seen=True):\n","    with Timer() as t:\n","        topk_scores = model.recommend_k_items(\n","            test, top_k=top_k, remove_seen=remove_seen\n","        )\n","    return topk_scores, t\n","\n","lightgcn_param = {\n","    #\"yaml_file\": os.path.join(\"drive\", \"MyDrive\", \"4222Group9\", \"lightgcn.yaml\"),\n","    \"yaml_file\": \"lightgcn.yaml\",\n","    \"n_layers\": 3,\n","    \"batch_size\": 1024,\n","    \"epochs\": 10,\n","    \"learning_rate\": 0.005,\n","    \"eval_epoch\": 5,\n","    \"top_k\": DEFAULT_K,\n","    \"stacking_func\": 1, #0: original, 1: exponential decay, 1.5: exponential increase, 2: trainable with random initialization, 3: trainable with unified initialization\n","}"]},{"cell_type":"markdown","metadata":{},"source":["### KGAT"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import sys\n","import random\n","from time import time\n","import os\n","\n","import pandas as pd\n","from tqdm import tqdm\n","import torch.optim as optim\n","import torch.nn as nn\n","\n","from KGAT_folder.KGAT import KGAT\n","from KGAT_folder.log_helper import *\n","from KGAT_folder.parser_kgat import *\n","from KGAT_folder.metrics import *\n","from KGAT_folder.model_helper import *\n","from KGAT_folder.loader_kgat import DataLoaderKGAT\n","\n","def train_kgat():\n","    args = parse_kgat_args()\n","    time0 = time()\n","    model, data, Ks, device = train(args)\n","    time1 = time()\n","    t = time1-time0\n","    return model, data, Ks, device, t\n","\n","def evaluate_kgat(model, dataloader, Ks, device):\n","    test_batch_size = dataloader.test_batch_size\n","    train_user_dict = dataloader.train_user_dict\n","    test_user_dict = dataloader.test_user_dict\n","\n","    model.eval()\n","\n","    user_ids = list(test_user_dict.keys())\n","    user_ids_batches = [user_ids[i: i + test_batch_size] for i in range(0, len(user_ids), test_batch_size)]\n","    user_ids_batches = [torch.LongTensor(d) for d in user_ids_batches]\n","\n","    n_items = dataloader.n_items\n","    item_ids = torch.arange(n_items, dtype=torch.long).to(device)\n","    \n","    cf_scores = []\n","    metric_names = ['precision', 'recall', 'ndcg']\n","    metrics_dict = {k: {m: [] for m in metric_names} for k in Ks}\n","\n","    with tqdm(total=len(user_ids_batches), desc='Evaluating Iteration') as pbar:\n","        for batch_user_ids in user_ids_batches:\n","            batch_user_ids = batch_user_ids.to(device)\n","\n","            with torch.no_grad():\n","                batch_scores = model(batch_user_ids, item_ids, mode='predict')       # (n_batch_users, n_items)\n","\n","            batch_scores = batch_scores.cpu()\n","            batch_metrics = calc_metrics_at_k(batch_scores, train_user_dict, test_user_dict, batch_user_ids.cpu().numpy(), item_ids.cpu().numpy(), Ks)\n","\n","            cf_scores.append(batch_scores.numpy())\n","            for k in Ks:\n","                for m in metric_names:\n","                    metrics_dict[k][m].append(batch_metrics[k][m])\n","            pbar.update(1)\n","\n","    cf_scores = np.concatenate(cf_scores, axis=0)\n","\n","    for k in Ks:\n","        for m in metric_names:\n","            metrics_dict[k][m] = np.concatenate(metrics_dict[k][m]).mean()\n","    return cf_scores, metrics_dict\n","\n","    def train(args):\n","    # seed\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    torch.cuda.manual_seed_all(args.seed)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    data = DataLoaderKGAT(args, logging)\n","    if args.use_pretrain == 1:\n","        user_pre_embed = torch.tensor(data.user_pre_embed)\n","        item_pre_embed = torch.tensor(data.item_pre_embed)\n","    else:\n","        user_pre_embed, item_pre_embed = None, None\n","    model = KGAT(args, data.n_users, data.n_entities, data.n_relations, data.A_in, user_pre_embed, item_pre_embed)\n","    model.to(device)\n","\n","    best_epochs = -1 # Can change the epochs here\n","\n","    ref_epochs = 10\n","    \n","\n","    cf_optimizer = optim.Adam(model.parameters(), lr=args.lr)\n","    kg_optimizer = optim.Adam(model.parameters(), lr=args.lr)\n","\n","    best_recalls = 0 # The optimiser will affect the values of the recall\n","\n","    ref_recalls = 50\n","\n","    Ks = eval(args.Ks)\n","    k_min = min(Ks)\n","    k_max = max(Ks)\n","\n","    epoch_list = []\n","    metrics_list = {k: {'precision': [], 'recall': [], 'ndcg': []} for k in Ks}\n","\n","    for epoch in range(1, args.n_epoch + 1):\n","        time0 = time()\n","        model.train()\n","        time1 = time()\n","        cf_total_loss = 0\n","        n_cf_batch = data.n_cf_train // data.cf_batch_size + 1\n","\n","        for iter in range(1, n_cf_batch + 1):\n","            time2 = time()\n","            cf_batch_user, cf_batch_pos_item, cf_batch_neg_item = data.generate_cf_batch(data.train_user_dict, data.cf_batch_size)\n","            cf_batch_user = cf_batch_user.to(device)\n","            cf_batch_pos_item = cf_batch_pos_item.to(device)\n","            cf_batch_neg_item = cf_batch_neg_item.to(device)\n","            cf_batch_loss = model(cf_batch_user, cf_batch_pos_item, cf_batch_neg_item, mode='train_cf')\n","\n","            if np.isnan(cf_batch_loss.cpu().detach().numpy()):\n","                logging.info('ERROR (CF Training): Epoch {:04d} Iter {:04d} / {:04d} Loss is nan.'.format(epoch, iter, n_cf_batch))\n","                sys.exit()\n","\n","            cf_batch_loss.backward()\n","            cf_optimizer.step()\n","            cf_optimizer.zero_grad()\n","            cf_total_loss += cf_batch_loss.item()\n","\n","            if (iter % args.cf_print_every) == 0:\n","                logging.info('CF Training: Epoch {:04d} Iter {:04d} / {:04d} | Time {:.1f}s | Iter Loss {:.4f} | Iter Mean Loss {:.4f}'.format(epoch, iter, n_cf_batch, time() - time2, cf_batch_loss.item(), cf_total_loss / iter))\n","        logging.info('CF Training: Epoch {:04d} Total Iter {:04d} | Total Time {:.1f}s | Iter Mean Loss {:.4f}'.format(epoch, n_cf_batch, time() - time1, cf_total_loss / n_cf_batch))\n","\n","        time3 = time()\n","        kg_total_loss = 0\n","        n_kg_batch = data.n_kg_train // data.kg_batch_size + 1\n","\n","        for iter in range(1, n_kg_batch + 1):\n","            time4 = time()\n","            kg_batch_head, kg_batch_relation, kg_batch_pos_tail, kg_batch_neg_tail = data.generate_kg_batch(data.train_kg_dict, data.kg_batch_size, data.n_users_entities)\n","            kg_batch_head = kg_batch_head.to(device)\n","            kg_batch_relation = kg_batch_relation.to(device)\n","            kg_batch_pos_tail = kg_batch_pos_tail.to(device)\n","            kg_batch_neg_tail = kg_batch_neg_tail.to(device)\n","\n","            kg_batch_loss = model(kg_batch_head, kg_batch_relation, kg_batch_pos_tail, kg_batch_neg_tail, mode='train_kg')\n","\n","            if np.isnan(kg_batch_loss.cpu().detach().numpy()):\n","                logging.info('ERROR (KG Training): Epoch {:04d} Iter {:04d} / {:04d} Loss is nan.'.format(epoch, iter, n_kg_batch))\n","                sys.exit()\n","\n","            kg_batch_loss.backward()\n","            kg_optimizer.step()\n","            kg_optimizer.zero_grad()\n","            kg_total_loss += kg_batch_loss.item()\n","\n","            if (iter % args.kg_print_every) == 0:\n","                logging.info('KG Training: Epoch {:04d} Iter {:04d} / {:04d} | Time {:.1f}s | Iter Loss {:.4f} | Iter Mean Loss {:.4f}'.format(epoch, iter, n_kg_batch, time() - time4, kg_batch_loss.item(), kg_total_loss / iter))\n","        logging.info('KG Training: Epoch {:04d} Total Iter {:04d} | Total Time {:.1f}s | Iter Mean Loss {:.4f}'.format(epoch, n_kg_batch, time() - time3, kg_total_loss / n_kg_batch))\n","\n","        time5 = time()\n","        h_list = data.h_list.to(device)\n","        t_list = data.t_list.to(device)\n","        r_list = data.r_list.to(device)\n","        relations = list(data.laplacian_dict.keys())\n","        model(h_list, t_list, r_list, relations, mode='update_att')\n","        logging.info('Update Attention: Epoch {:04d} | Total Time {:.1f}s'.format(epoch, time() - time5))\n","\n","        logging.info('CF + KG Training: Epoch {:04d} | Total Time {:.1f}s'.format(epoch, time() - time0))\n","    return model, data, Ks, device\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Prepare for training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","from recommenders.evaluation.python_evaluation import (\n","    map_at_k,\n","    ndcg_at_k,\n","    precision_at_k,\n","    recall_at_k,\n",")\n","def ranking_metrics_python(test, predictions, k=DEFAULT_K):\n","    return {\n","        \"MAP\": map_at_k(test, predictions, k=k, **COL_DICT),\n","        \"nDCG@k\": ndcg_at_k(test, predictions, k=k, **COL_DICT),\n","        \"Precision@k\": precision_at_k(test, predictions, k=k, **COL_DICT),\n","        \"Recall@k\": recall_at_k(test, predictions, k=k, **COL_DICT),\n","    }\n","\n","#params = {\n","#    \"als\": als_params,\n","#    \"ncf\": ncf_params,\n","#    \"lightgcn\": lightgcn_param,\n","#}\n","prepare_training_data = {\n","    \"als\": prepare_training_als,\n","    \"ncf\": prepare_training_ncf,\n","    \"lightgcn\": prepare_training_lightgcn,\n","}\n","\n","\n","from recommenders.evaluation.spark_evaluation import (\n","    SparkRatingEvaluation,\n","    SparkRankingEvaluation,\n",")\n","def rating_metrics_pyspark(test, predictions):\n","    rating_eval = SparkRatingEvaluation(test, predictions, **COL_DICT)\n","    return {\n","        \"RMSE\": rating_eval.rmse(),\n","        \"MAE\": rating_eval.mae(),\n","        \"R2\": rating_eval.exp_var(),\n","        \"Explained Variance\": rating_eval.rsquared(),\n","    }\n","def ranking_metrics_pyspark(test, predictions, k=DEFAULT_K):\n","    rank_eval = SparkRankingEvaluation(\n","        test, predictions, k=k, relevancy_method=\"top_k\", **COL_DICT\n","    )\n","    return {\n","        \"MAP\": rank_eval.map_at_k(),\n","        \"nDCG@k\": rank_eval.ndcg_at_k(),\n","        \"Precision@k\": rank_eval.precision_at_k(),\n","        \"Recall@k\": rank_eval.recall_at_k(),\n","    }\n","\n","trainer = {\n","    \"als\": lambda params, data: train_als(params, data),\n","    \"ncf\": lambda params, data: train_ncf(params, data),\n","    \"lightgcn\": lambda params, data: train_lightgcn(params, data),\n","}\n","ranking_predictor = {\n","    \"als\": lambda model, test, train: recommend_k_als(model, test, train),\n","    \"ncf\": lambda model, test, train: recommend_k_ncf(model, test, train),\n","    \"lightgcn\": lambda model, test, train: recommend_k_lightgcn(model, test, train),\n","}\n","ranking_evaluator = {\n","    \"als\": lambda test, predictions, k: ranking_metrics_pyspark(test, predictions, k),\n","    \"ncf\": lambda test, predictions, k: ranking_metrics_python(test, predictions, k),\n","    \"lightgcn\": lambda test, predictions, k: ranking_metrics_python(test, predictions, k),\n","}\n","metrics = {\n","    #\"als\": [\"rating\", \"ranking\"],\n","    \"als\" : [\"ranking\"],\n","    \"ncf\": [\"ranking\"],\n","    \"lightgcn\": [\"ranking\"]\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1667642261029,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"WDnMUwTkmaVl"},"outputs":[],"source":["from recommenders.evaluation.spark_evaluation import (\n","    SparkRatingEvaluation,\n","    SparkRankingEvaluation,\n",")\n","def rating_metrics_pyspark(test, predictions):\n","    rating_eval = SparkRatingEvaluation(test, predictions, **COL_DICT)\n","    return {\n","        \"RMSE\": rating_eval.rmse(),\n","        \"MAE\": rating_eval.mae(),\n","        \"R2\": rating_eval.exp_var(),\n","        \"Explained Variance\": rating_eval.rsquared(),\n","    }\n","def ranking_metrics_pyspark(test, predictions, k=DEFAULT_K):\n","    rank_eval = SparkRankingEvaluation(\n","        test, predictions, k=k, relevancy_method=\"top_k\", **COL_DICT\n","    )\n","    return {\n","        \"MAP\": rank_eval.map_at_k(),\n","        \"nDCG@k\": rank_eval.ndcg_at_k(),\n","        \"Precision@k\": rank_eval.precision_at_k(),\n","        \"Recall@k\": rank_eval.recall_at_k(),\n","    }\n","trainer = {\n","    \"lightgcn\": lambda params, data: train_lightgcn(params, data),\n","}\n","ranking_predictor = {\n","    \"lightgcn\": lambda model, test, train: recommend_k_lightgcn(model, test, train),\n","}\n","ranking_evaluator = {\n","    \"lightgcn\": lambda test, predictions, k: ranking_metrics_python(test, predictions, k),\n","}\n","metrics = {\n","    \"lightgcn\": [\"ranking\"]\n","}"]},{"cell_type":"markdown","metadata":{},"source":["## Sanity Check by Overfitting on Small Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#!pip install pytorch-lightning\n","#from pytorch_lightning import Trainer, seed_everything\n","#from LightGCN.code.main_lgcn import sanity_check\n","\n","#seed_everything(42, workers=True)\n","\n","#model = sanity_check()\n","#trainer = Trainer(max_epochs=10000, overfit_batches=0.01)\n","#trainer.fit(model)"]},{"cell_type":"markdown","metadata":{"id":"3B4ScYiXzCN1"},"source":["# 6 Hyperparameter Tunning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def generate_summary(data, algo, k, train_time, time_rating, rating_metrics, time_ranking, ranking_metrics):\n","    summary = {\"Data\": data, \"K\": k, \"Train time (s)\": train_time, \"Recommending time (s)\": time_ranking}\n","    if rating_metrics is None:\n","        rating_metrics = {\n","            \"RMSE\": np.nan,\n","            \"MAE\": np.nan,\n","            \"R2\": np.nan,\n","            \"Explained Variance\": np.nan,\n","        }\n","    if ranking_metrics is None:\n","        ranking_metrics = {\n","            \"MAP\": np.nan,\n","            \"nDCG@k\": np.nan,\n","            \"Precision@k\": np.nan,\n","            \"Recall@k\": np.nan,\n","        }\n","    summary.update(ranking_metrics)\n","    return summary"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data_sizes = [\"100k\"] # Movielens data size: 100k, 1m, 10m, or 20m\n","algorithms = [ \"lightgcn\"]\n","\n","lightgcn_param_dict = {\n","    #\"yaml_file\": os.path.join(\"drive\", \"MyDrive\", \"4222Group9\", \"lightgcn.yaml\"),\n","    \"yaml_file\": \"lightgcn.yaml\",\n","    \"embed_size\" : [32, 64, 128], # the embedding dimension of users and items\n","    # \"n_layers\": 3, # number of layers of the model\n","\n","    # \"batch_size\": 1024,\n","    \"decay\" : 0.0001, # l2 regularization for embedding parameters\n","    \"epochs\": 200,\n","    \"learning_rate\": [0.0005, 0.001, 0.002],\n","    \"eval_epoch\": 50,\n","    # \"top_k\": DEFAULT_K,\n","    \"stacking_func\": [0,1,1.5], #0: original, 1: exponential decay, 1.5: exponential increase, 2: trainable with random initialization, 3: trainable with unified initialization\n","\n","    # \"save_model\" : False,\n","    # \"save_epoch\" : 10,\n","    \"save_board\": True,\n","    \"board_comment\": \"\",\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from recommenders.tuning.parameter_sweep import generate_param_grid\n","lightgcn_params = generate_param_grid(lightgcn_param_dict)"]},{"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import traceback\n","from recommenders.datasets import movielens\n","from recommenders.datasets.python_splitters import python_stratified_split"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","data_sizes = [\"100k\"] # Movielens data size: 100k, 1m, 10m, or 20m\n","algorithms = [ \"lightgcn\"]\n","\n","# For each data size and each algorithm, a recommender is evaluated. \n","cols = [\"Data\", \"Algo\", \"K\", \"Train time (s)\", \"Predicting time (s)\",  \"Recommending time (s)\", \"MAP\", \"nDCG@k\", \"Precision@k\", \"Recall@k\"]\n","df_results = pd.DataFrame(columns=cols)\n","\n","for data_size in data_sizes:\n","\n","    # Load the dataset\n","    #df = movielens.load_pandas_df(\n","    #    size=data_size,\n","    #    header=[DEFAULT_USER_COL, DEFAULT_ITEM_COL, DEFAULT_RATING_COL, DEFAULT_TIMESTAMP_COL]\n","    #)\n","    if data_size=='100k':\n","        df = pd.read_csv('ml-100k/u.data', sep='\\t', header=None)\n","\n","    df.columns = [DEFAULT_USER_COL, DEFAULT_ITEM_COL, DEFAULT_RATING_COL, DEFAULT_TIMESTAMP_COL]\n","    df[DEFAULT_RATING_COL] = df[DEFAULT_RATING_COL].astype(float)\n","    print(\"Size of Movielens {}: {}\".format(data_size, df.shape))\n","    \n","    # Split the dataset\n","    df_train, df_test = python_stratified_split(df,\n","                                                ratio=0.75, \n","                                                min_rating=1, \n","                                                filter_by=\"item\", \n","                                                col_user=DEFAULT_USER_COL, \n","                                                col_item=DEFAULT_ITEM_COL,\n","                                                )\n","\n","    #df_train = \"../LightGCN/data/movielens/train.txt\"\n","    # Loop through the algos\n","    for algo in algorithms:\n","        print(f\"\\nComputing {algo} algorithm on Movielens {data_size}\")\n","        if algo == 'kgat':\n","            model, data, Ks, device, time_train = train_kgat()\n","            _, metrics_dict_kgat = evaluate_kgat(model, data, Ks, device)\n","            print(metrics_dict_kgat)\n","            # Record results\n","            #summary = generate_summary('100k', algo, DEFAULT_K, time_train, time_rating, ratings, time_ranking, rankings)\n","            #df_results.loc[df_results.shape[0] + 1] = summary\n","            \n","        else:\n","            # Data prep for training set\n","            obj = prepare_training_data.get(algo, lambda x,y:(x,y))(df_train, df_test)\n","            \n","            for model_params in lightgcn_params:\n","                print(\"training using the params:\", model_params)\n","                model_params[\"board_comment\"] = str(model_params)\n","\n","                # Train the model\n","                #try:\n","                model, time_train = trainer[algo](model_params, obj)\n","                #except:\n","                    #traceback.print_exc()\n","                \n","                print(f\"Training time: {time_train}s\")\n","                        \n","                # Predict and evaluate\n","                #train, test = prepare_metrics_data.get(algo, lambda x,y:(x,y))(df_train, df_test)\n","                train, test = df_train, df_test\n","                \n","                if \"rating\" in metrics[algo]:   \n","                    pass\n","                else:\n","                    ratings = None\n","                    time_rating = np.nan\n","                \n","                if \"ranking\" in metrics[algo]:\n","                    # Predict for ranking\n","                    top_k_scores, time_ranking = ranking_predictor[algo](model, test, train)\n","                    print(f\"Ranking prediction time: {time_ranking}s\")\n","                    \n","                    # Evaluate for rating\n","                    rankings = ranking_evaluator[algo](test, top_k_scores, DEFAULT_K)\n","                else:\n","                    rankings = None\n","                    time_ranking = np.nan\n","                    \n","                # Record results\n","                summary = generate_summary(data_size, algo, DEFAULT_K, time_train, time_rating, ratings, time_ranking, rankings)\n","                df_results.loc[df_results.shape[0] + 1] = summary\n","        \n","print(\"\\nComputation finished\")"]},{"cell_type":"markdown","metadata":{},"source":["## Training Result Plot"]},{"cell_type":"markdown","metadata":{},"source":["Check the result table, and then filter on that table to see suitable params setting"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'df_results' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_result \u001b[38;5;241m=\u001b[39m \u001b[43mdf_results\u001b[49m\n\u001b[1;32m      2\u001b[0m df_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparam\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m lightgcn_params\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pd\u001b[38;5;241m.\u001b[39moption_context(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_rows\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n","\u001b[0;31mNameError\u001b[0m: name 'df_results' is not defined"]}],"source":["df_result = df_results\n","df_result['param'] = lightgcn_params\n","with pd.option_context('display.max_rows', None):\n","    with pd.option_context('display.max_colwidth', None):\n","        df_result.to_csv('params.csv')\n","        #from IPython.display import HTML\n","        #HTML(df_result.to_html())\n","        df_result"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Data</th>\n","      <th>Algo</th>\n","      <th>K</th>\n","      <th>Train time (s)</th>\n","      <th>Predicting time (s)</th>\n","      <th>Recommending time (s)</th>\n","      <th>MAP</th>\n","      <th>nDCG@k</th>\n","      <th>Precision@k</th>\n","      <th>Recall@k</th>\n","      <th>param</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>100k</td>\n","      <td>lightgcn</td>\n","      <td>10</td>\n","      <td>148.77</td>\n","      <td>NaN</td>\n","      <td>0.10</td>\n","      <td>0.12</td>\n","      <td>0.41</td>\n","      <td>0.35</td>\n","      <td>0.19</td>\n","      <td>{'embed_size': 32, 'learning_rate': 0.0005, 's...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>100k</td>\n","      <td>lightgcn</td>\n","      <td>10</td>\n","      <td>152.58</td>\n","      <td>NaN</td>\n","      <td>0.12</td>\n","      <td>0.13</td>\n","      <td>0.44</td>\n","      <td>0.38</td>\n","      <td>0.21</td>\n","      <td>{'embed_size': 32, 'learning_rate': 0.0005, 's...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>100k</td>\n","      <td>lightgcn</td>\n","      <td>10</td>\n","      <td>154.85</td>\n","      <td>NaN</td>\n","      <td>0.09</td>\n","      <td>0.06</td>\n","      <td>0.22</td>\n","      <td>0.20</td>\n","      <td>0.12</td>\n","      <td>{'embed_size': 32, 'learning_rate': 0.0005, 's...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>100k</td>\n","      <td>lightgcn</td>\n","      <td>10</td>\n","      <td>150.40</td>\n","      <td>NaN</td>\n","      <td>0.13</td>\n","      <td>0.13</td>\n","      <td>0.44</td>\n","      <td>0.38</td>\n","      <td>0.21</td>\n","      <td>{'embed_size': 32, 'learning_rate': 0.001, 'st...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>100k</td>\n","      <td>lightgcn</td>\n","      <td>10</td>\n","      <td>154.14</td>\n","      <td>NaN</td>\n","      <td>0.09</td>\n","      <td>0.14</td>\n","      <td>0.45</td>\n","      <td>0.39</td>\n","      <td>0.21</td>\n","      <td>{'embed_size': 32, 'learning_rate': 0.001, 'st...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>23</td>\n","      <td>100k</td>\n","      <td>lightgcn</td>\n","      <td>10</td>\n","      <td>166.56</td>\n","      <td>NaN</td>\n","      <td>0.11</td>\n","      <td>0.13</td>\n","      <td>0.43</td>\n","      <td>0.38</td>\n","      <td>0.21</td>\n","      <td>{'embed_size': 128, 'learning_rate': 0.001, 's...</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>24</td>\n","      <td>100k</td>\n","      <td>lightgcn</td>\n","      <td>10</td>\n","      <td>174.41</td>\n","      <td>NaN</td>\n","      <td>0.11</td>\n","      <td>0.06</td>\n","      <td>0.23</td>\n","      <td>0.21</td>\n","      <td>0.12</td>\n","      <td>{'embed_size': 128, 'learning_rate': 0.001, 's...</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>25</td>\n","      <td>100k</td>\n","      <td>lightgcn</td>\n","      <td>10</td>\n","      <td>169.75</td>\n","      <td>NaN</td>\n","      <td>0.12</td>\n","      <td>0.13</td>\n","      <td>0.44</td>\n","      <td>0.39</td>\n","      <td>0.22</td>\n","      <td>{'embed_size': 128, 'learning_rate': 0.002, 's...</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>26</td>\n","      <td>100k</td>\n","      <td>lightgcn</td>\n","      <td>10</td>\n","      <td>168.98</td>\n","      <td>NaN</td>\n","      <td>0.12</td>\n","      <td>0.10</td>\n","      <td>0.35</td>\n","      <td>0.31</td>\n","      <td>0.17</td>\n","      <td>{'embed_size': 128, 'learning_rate': 0.002, 's...</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>27</td>\n","      <td>100k</td>\n","      <td>lightgcn</td>\n","      <td>10</td>\n","      <td>173.93</td>\n","      <td>NaN</td>\n","      <td>0.11</td>\n","      <td>0.06</td>\n","      <td>0.23</td>\n","      <td>0.20</td>\n","      <td>0.12</td>\n","      <td>{'embed_size': 128, 'learning_rate': 0.002, 's...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>27 rows Ã— 12 columns</p>\n","</div>"],"text/plain":["    Unnamed: 0  Data      Algo   K  Train time (s)  Predicting time (s)  \\\n","0            1  100k  lightgcn  10          148.77                  NaN   \n","1            2  100k  lightgcn  10          152.58                  NaN   \n","2            3  100k  lightgcn  10          154.85                  NaN   \n","3            4  100k  lightgcn  10          150.40                  NaN   \n","4            5  100k  lightgcn  10          154.14                  NaN   \n","..         ...   ...       ...  ..             ...                  ...   \n","22          23  100k  lightgcn  10          166.56                  NaN   \n","23          24  100k  lightgcn  10          174.41                  NaN   \n","24          25  100k  lightgcn  10          169.75                  NaN   \n","25          26  100k  lightgcn  10          168.98                  NaN   \n","26          27  100k  lightgcn  10          173.93                  NaN   \n","\n","    Recommending time (s)  MAP  nDCG@k  Precision@k  Recall@k  \\\n","0                    0.10 0.12    0.41         0.35      0.19   \n","1                    0.12 0.13    0.44         0.38      0.21   \n","2                    0.09 0.06    0.22         0.20      0.12   \n","3                    0.13 0.13    0.44         0.38      0.21   \n","4                    0.09 0.14    0.45         0.39      0.21   \n","..                    ...  ...     ...          ...       ...   \n","22                   0.11 0.13    0.43         0.38      0.21   \n","23                   0.11 0.06    0.23         0.21      0.12   \n","24                   0.12 0.13    0.44         0.39      0.22   \n","25                   0.12 0.10    0.35         0.31      0.17   \n","26                   0.11 0.06    0.23         0.20      0.12   \n","\n","                                                param  \n","0   {'embed_size': 32, 'learning_rate': 0.0005, 's...  \n","1   {'embed_size': 32, 'learning_rate': 0.0005, 's...  \n","2   {'embed_size': 32, 'learning_rate': 0.0005, 's...  \n","3   {'embed_size': 32, 'learning_rate': 0.001, 'st...  \n","4   {'embed_size': 32, 'learning_rate': 0.001, 'st...  \n","..                                                ...  \n","22  {'embed_size': 128, 'learning_rate': 0.001, 's...  \n","23  {'embed_size': 128, 'learning_rate': 0.001, 's...  \n","24  {'embed_size': 128, 'learning_rate': 0.002, 's...  \n","25  {'embed_size': 128, 'learning_rate': 0.002, 's...  \n","26  {'embed_size': 128, 'learning_rate': 0.002, 's...  \n","\n","[27 rows x 12 columns]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["df_result = pd.read_csv('params.csv')\n","df_result"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Data</th>\n","      <th>Algo</th>\n","      <th>K</th>\n","      <th>Train time (s)</th>\n","      <th>Predicting time (s)</th>\n","      <th>Recommending time (s)</th>\n","      <th>MAP</th>\n","      <th>nDCG@k</th>\n","      <th>Precision@k</th>\n","      <th>Recall@k</th>\n","      <th>param</th>\n","      <th>embed_size</th>\n","      <th>learning_rate</th>\n","      <th>stacking_func</th>\n","      <th>yaml_file</th>\n","      <th>decay</th>\n","      <th>epochs</th>\n","      <th>eval_epoch</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>100k</td>\n","      <td>lightgcn</td>\n","      <td>10</td>\n","      <td>148.77</td>\n","      <td>NaN</td>\n","      <td>0.10</td>\n","      <td>0.12</td>\n","      <td>0.41</td>\n","      <td>0.35</td>\n","      <td>0.19</td>\n","      <td>{'embed_size': 32, 'learning_rate': 0.0005, 's...</td>\n","      <td>32.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>lightgcn.yaml</td>\n","      <td>0.00</td>\n","      <td>200.00</td>\n","      <td>50.00</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>100k</td>\n","      <td>lightgcn</td>\n","      <td>10</td>\n","      <td>152.58</td>\n","      <td>NaN</td>\n","      <td>0.12</td>\n","      <td>0.13</td>\n","      <td>0.44</td>\n","      <td>0.38</td>\n","      <td>0.21</td>\n","      <td>{'embed_size': 32, 'learning_rate': 0.0005, 's...</td>\n","      <td>32.00</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>lightgcn.yaml</td>\n","      <td>0.00</td>\n","      <td>200.00</td>\n","      <td>50.00</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>100k</td>\n","      <td>lightgcn</td>\n","      <td>10</td>\n","      <td>154.85</td>\n","      <td>NaN</td>\n","      <td>0.09</td>\n","      <td>0.06</td>\n","      <td>0.22</td>\n","      <td>0.20</td>\n","      <td>0.12</td>\n","      <td>{'embed_size': 32, 'learning_rate': 0.0005, 's...</td>\n","      <td>32.00</td>\n","      <td>0.00</td>\n","      <td>1.50</td>\n","      <td>lightgcn.yaml</td>\n","      <td>0.00</td>\n","      <td>200.00</td>\n","      <td>50.00</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>100k</td>\n","      <td>lightgcn</td>\n","      <td>10</td>\n","      <td>150.40</td>\n","      <td>NaN</td>\n","      <td>0.13</td>\n","      <td>0.13</td>\n","      <td>0.44</td>\n","      <td>0.38</td>\n","      <td>0.21</td>\n","      <td>{'embed_size': 32, 'learning_rate': 0.001, 'st...</td>\n","      <td>32.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>lightgcn.yaml</td>\n","      <td>0.00</td>\n","      <td>200.00</td>\n","      <td>50.00</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>100k</td>\n","      <td>lightgcn</td>\n","      <td>10</td>\n","      <td>154.14</td>\n","      <td>NaN</td>\n","      <td>0.09</td>\n","      <td>0.14</td>\n","      <td>0.45</td>\n","      <td>0.39</td>\n","      <td>0.21</td>\n","      <td>{'embed_size': 32, 'learning_rate': 0.001, 'st...</td>\n","      <td>32.00</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>lightgcn.yaml</td>\n","      <td>0.00</td>\n","      <td>200.00</td>\n","      <td>50.00</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>23</td>\n","      <td>100k</td>\n","      <td>lightgcn</td>\n","      <td>10</td>\n","      <td>166.56</td>\n","      <td>NaN</td>\n","      <td>0.11</td>\n","      <td>0.13</td>\n","      <td>0.43</td>\n","      <td>0.38</td>\n","      <td>0.21</td>\n","      <td>{'embed_size': 128, 'learning_rate': 0.001, 's...</td>\n","      <td>128.00</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>lightgcn.yaml</td>\n","      <td>0.00</td>\n","      <td>200.00</td>\n","      <td>50.00</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>24</td>\n","      <td>100k</td>\n","      <td>lightgcn</td>\n","      <td>10</td>\n","      <td>174.41</td>\n","      <td>NaN</td>\n","      <td>0.11</td>\n","      <td>0.06</td>\n","      <td>0.23</td>\n","      <td>0.21</td>\n","      <td>0.12</td>\n","      <td>{'embed_size': 128, 'learning_rate': 0.001, 's...</td>\n","      <td>128.00</td>\n","      <td>0.00</td>\n","      <td>1.50</td>\n","      <td>lightgcn.yaml</td>\n","      <td>0.00</td>\n","      <td>200.00</td>\n","      <td>50.00</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>25</td>\n","      <td>100k</td>\n","      <td>lightgcn</td>\n","      <td>10</td>\n","      <td>169.75</td>\n","      <td>NaN</td>\n","      <td>0.12</td>\n","      <td>0.13</td>\n","      <td>0.44</td>\n","      <td>0.39</td>\n","      <td>0.22</td>\n","      <td>{'embed_size': 128, 'learning_rate': 0.002, 's...</td>\n","      <td>128.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>lightgcn.yaml</td>\n","      <td>0.00</td>\n","      <td>200.00</td>\n","      <td>50.00</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>26</td>\n","      <td>100k</td>\n","      <td>lightgcn</td>\n","      <td>10</td>\n","      <td>168.98</td>\n","      <td>NaN</td>\n","      <td>0.12</td>\n","      <td>0.10</td>\n","      <td>0.35</td>\n","      <td>0.31</td>\n","      <td>0.17</td>\n","      <td>{'embed_size': 128, 'learning_rate': 0.002, 's...</td>\n","      <td>128.00</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>lightgcn.yaml</td>\n","      <td>0.00</td>\n","      <td>200.00</td>\n","      <td>50.00</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>27</td>\n","      <td>100k</td>\n","      <td>lightgcn</td>\n","      <td>10</td>\n","      <td>173.93</td>\n","      <td>NaN</td>\n","      <td>0.11</td>\n","      <td>0.06</td>\n","      <td>0.23</td>\n","      <td>0.20</td>\n","      <td>0.12</td>\n","      <td>{'embed_size': 128, 'learning_rate': 0.002, 's...</td>\n","      <td>128.00</td>\n","      <td>0.00</td>\n","      <td>1.50</td>\n","      <td>lightgcn.yaml</td>\n","      <td>0.00</td>\n","      <td>200.00</td>\n","      <td>50.00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>27 rows Ã— 19 columns</p>\n","</div>"],"text/plain":["    Unnamed: 0  Data      Algo   K  Train time (s)  Predicting time (s)  \\\n","0            1  100k  lightgcn  10          148.77                  NaN   \n","1            2  100k  lightgcn  10          152.58                  NaN   \n","2            3  100k  lightgcn  10          154.85                  NaN   \n","3            4  100k  lightgcn  10          150.40                  NaN   \n","4            5  100k  lightgcn  10          154.14                  NaN   \n","..         ...   ...       ...  ..             ...                  ...   \n","22          23  100k  lightgcn  10          166.56                  NaN   \n","23          24  100k  lightgcn  10          174.41                  NaN   \n","24          25  100k  lightgcn  10          169.75                  NaN   \n","25          26  100k  lightgcn  10          168.98                  NaN   \n","26          27  100k  lightgcn  10          173.93                  NaN   \n","\n","    Recommending time (s)  MAP  nDCG@k  Precision@k  Recall@k  \\\n","0                    0.10 0.12    0.41         0.35      0.19   \n","1                    0.12 0.13    0.44         0.38      0.21   \n","2                    0.09 0.06    0.22         0.20      0.12   \n","3                    0.13 0.13    0.44         0.38      0.21   \n","4                    0.09 0.14    0.45         0.39      0.21   \n","..                    ...  ...     ...          ...       ...   \n","22                   0.11 0.13    0.43         0.38      0.21   \n","23                   0.11 0.06    0.23         0.21      0.12   \n","24                   0.12 0.13    0.44         0.39      0.22   \n","25                   0.12 0.10    0.35         0.31      0.17   \n","26                   0.11 0.06    0.23         0.20      0.12   \n","\n","                                                param  embed_size  \\\n","0   {'embed_size': 32, 'learning_rate': 0.0005, 's...       32.00   \n","1   {'embed_size': 32, 'learning_rate': 0.0005, 's...       32.00   \n","2   {'embed_size': 32, 'learning_rate': 0.0005, 's...       32.00   \n","3   {'embed_size': 32, 'learning_rate': 0.001, 'st...       32.00   \n","4   {'embed_size': 32, 'learning_rate': 0.001, 'st...       32.00   \n","..                                                ...         ...   \n","22  {'embed_size': 128, 'learning_rate': 0.001, 's...      128.00   \n","23  {'embed_size': 128, 'learning_rate': 0.001, 's...      128.00   \n","24  {'embed_size': 128, 'learning_rate': 0.002, 's...      128.00   \n","25  {'embed_size': 128, 'learning_rate': 0.002, 's...      128.00   \n","26  {'embed_size': 128, 'learning_rate': 0.002, 's...      128.00   \n","\n","    learning_rate  stacking_func      yaml_file  decay  epochs  eval_epoch  \n","0            0.00           0.00  lightgcn.yaml   0.00  200.00       50.00  \n","1            0.00           1.00  lightgcn.yaml   0.00  200.00       50.00  \n","2            0.00           1.50  lightgcn.yaml   0.00  200.00       50.00  \n","3            0.00           0.00  lightgcn.yaml   0.00  200.00       50.00  \n","4            0.00           1.00  lightgcn.yaml   0.00  200.00       50.00  \n","..            ...            ...            ...    ...     ...         ...  \n","22           0.00           1.00  lightgcn.yaml   0.00  200.00       50.00  \n","23           0.00           1.50  lightgcn.yaml   0.00  200.00       50.00  \n","24           0.00           0.00  lightgcn.yaml   0.00  200.00       50.00  \n","25           0.00           1.00  lightgcn.yaml   0.00  200.00       50.00  \n","26           0.00           1.50  lightgcn.yaml   0.00  200.00       50.00  \n","\n","[27 rows x 19 columns]"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["for key, value in eval(df_result.at[0, \"param\"]).items():\n","    for i in np.arange(df_result.shape[0]):\n","        df_result.at[i, key] = eval(df_result.at[i, \"param\"])[key]\n","df_result"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.1175262640396166 0.2273533123813273\n"]},{"data":{"text/plain":["'temp-plot.html'"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["import plotly\n","import plotly.graph_objs as go\n","\n","recall = df_result['Recall@k']\n","print(recall.min(), recall.max())\n","recall = (recall - recall.min()) / (recall.max() - recall.min())\n","\n","fig1 = go.Scatter3d(x=df_result['embed_size'],\n","                    y=df_result['learning_rate'],\n","                    z=df_result['stacking_func'],\n","                    text = [item for item in recall.astype(str)],\n","                    marker=dict(color = recall,\n","                                reversescale=False,\n","                                colorscale='Blues',\n","                                size=7),\n","                    line=dict (width=0.02),\n","                    mode='markers')\n","\n","#Make Plot.ly Layout\n","mylayout = go.Layout(scene=dict(xaxis=dict(title=\"embed_size\"),\n","                                yaxis=dict(title=\"learning_rate\"),\n","                                zaxis=dict(title=\"stacking_func\")),)\n","\n","#Plot and save html\n","plotly.offline.plot({\"data\": [fig1],\n","                     \"layout\": mylayout},\n","                     auto_open=True)\n","# reference: https://medium.com/@prasadostwal/multi-dimension-plots-in-python-from-2d-to-6d-9a2bf7b8cc74"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lightgcn_params # see the corresponding param"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%load_ext tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%tensorboard --logdir logs/board"]},{"cell_type":"markdown","metadata":{},"source":["Upload the satisfactory result of tensorboard(change the name fit into folder name):"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!tensorboard dev upload \\\n","  --logdir logs/fit \\\n","  --name \"BiasedGCN experiment\" \\\n","  --description \"(optional) Simple comparison of several hyperparameters\" \\\n","  --one_shot"]},{"cell_type":"markdown","metadata":{},"source":["# 7 Comparisons on Movielens and Movie"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1667642261031,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"8reKYmEH2d-k"},"outputs":[],"source":["data_sizes = [\"100k\",\"1m\"] # Movielens data size: 100k, 1m, 10m, or 20m\n","#algorithms = [ \"lightgcn\"]\n","algorithms = [\"als\", \"ncf\", \"lightgcn\", \"kgat\"]"]},{"cell_type":"markdown","metadata":{},"source":["## Print the result summary"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"aborted","timestamp":1667642261031,"user":{"displayName":"Yj WEI","userId":"09205277183220707996"},"user_tz":-480},"id":"36QRfkqVrYzi"},"outputs":[],"source":["df_results"]},{"cell_type":"markdown","metadata":{"id":"seUWygXTzYfc"},"source":["# 8 Credit and Reference\n","\n","1. https://github.com/microsoft/recommenders"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python [conda env:cu101]","language":"python","name":"conda-env-cu101-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"},"vscode":{"interpreter":{"hash":"8006cd3f4b5a31b91c77e5b5a521798c8412e8b93831dc7973f670e0263ddfcb"}}},"nbformat":4,"nbformat_minor":0}
